{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-humidity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Deep Generative Models</center>\n",
    "## <center>Seminar 3</center>\n",
    "\n",
    "<center><img src=\"pics/ozonmasterslogo.png\" width=600 height=600 /></center>\n",
    "<center>25.02.2021</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-polyester",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plan\n",
    "- PCA\n",
    "- pPCA\n",
    "- VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-facility",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "$X \\in \\mathbb{R}^{D}$. We want to do some dimention reduction. How?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-bathroom",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's project the data to subspace having dimentionality $M < D$ while maximizing the variance of the projected data:\n",
    "\n",
    "$\\overline{x} = \\dfrac{1}{n}\\sum_{i=1}^{n}x_i$ - mean $x - D \\times 1$\n",
    "\n",
    "$S = \\dfrac{1}{n}\\sum_{i=1}^{n}(x_i - \\overline{x})(x_i - \\overline{x})^T$ - covariance matrix $D \\times D$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-ballet",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $M = 1$, so we want find $u_1$, $u_1^T u_1 = 1$, $u_1^T x_i \\in \\mathbb{R}$.\n",
    "\n",
    "$\\dfrac{1}{n}\\sum_{i=1}^{n}(u_1^T x_i - u_1^T \\overline{x})^2 = u_1^T S u_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-alabama",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want $u_1^T S u_1 \\rightarrow \\max_{u_1}$ and $u_1^T u_1 = 1$:\n",
    "\n",
    "We can define an optimization problem:\n",
    "\n",
    "$\\max_{u_1} u_1^T S u_1 + \\lambda_1(1 - u_1^T u_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-tumor",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By setting the derivative with respect to $u_1$ equal to zero we'll find:\n",
    "\n",
    "$S u_1 = \\lambda_1 u_1 \\Rightarrow u_1$ - eigenvector of $S$\n",
    "\n",
    "$u_1^T S u_1 = \\lambda_1 \\Rightarrow \\lambda_1$ - the largest eigenvalue of $S$\n",
    "\n",
    "Eigenvector $u_1$ is known as the first principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-facial",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can define additional principal components orthogonal to $u_1$ solving the same problem, it will be next eigenvectors $u_2, \\dots, u_M$.\n",
    "\n",
    "Why $u_i^T u_j = \\delta_{ij}?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-harvey",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also formulate the same task as reconstruction loss minimization task:\n",
    "\n",
    "$J = \\dfrac{1}{n}\\sum_{i=1}^{n} ||x_i - \\widetilde{x_i}||^2 \\rightarrow \\min_U$\n",
    "\n",
    "And get the same solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-walter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Should we normalize the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-sugar",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the computational cost of computing the full eigenvector decomposition for a matrix is $O(D^3)$\n",
    "- There is more efficient techniques, such as the power method (Golub and Van Loan, 1996) $O(M D^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-chorus",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilistic PCA\n",
    "\n",
    "$p(z) = N(z|0, I)$, $z \\in \\mathbb{R}^M$\n",
    "\n",
    "$p(x|z) = N(x|Wz + \\mu, \\sigma^2 I)$, $x, \\mu \\in \\mathbb{R}^D$, $W$ - $D \\times M$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-helicopter",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$x = Wz + \\mu + \\varepsilon, \\varepsilon \\sim N(0, \\sigma^2I)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-letters",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/ppca.jpg\" width=1000 height=1000 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-apollo",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$p(x) = \\int p(x|z)p(z) dz$ \n",
    "\n",
    "$p(x) = N(x| \\mu, C)$, $C = W W^T + \\sigma^2 I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-narrative",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\mathbb{E} x = \\mathbb{E} (Wz + \\mu + \\varepsilon) = \\mu$\n",
    "\n",
    "$cov(x) = \\mathbb{E} (Wz+\\varepsilon)(Wz+\\varepsilon)^T = \\mathbb{E} Wzz^T W^T + \\mathbb{E}\\varepsilon\\varepsilon^T = WW^T + \\sigma^2I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-fiction",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\widetilde{W} = WR$, $R$ - orthogonal matrix (rotations in the latent space)\n",
    "\n",
    "$C = WRR^TW^T + \\sigma^2I = WW^T + \\sigma^2I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-emission",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define $M = W^T W + \\sigma^2 I$\n",
    "\n",
    "$C^{-1} = \\sigma^{-1} I - \\sigma^{-2} W M^{-1} W^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-tracker",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(x)$ and $p(x|z)$ are Gaussian, so:\n",
    "\n",
    "$p(z|x) = N(z|M^{-1}W(x-\\mu), \\sigma^{-2}M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-oklahoma",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, using MLE we can find:\n",
    "\n",
    "$\\ln p(X|W, \\mu, \\sigma) = -\\dfrac{nD}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln |C| \\sum_{i=1}^{n}(x_i-\\mu)C^{-1}(x_i-\\mu)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-evanescence",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly:\n",
    "\n",
    "$W_{ML} = U_{M}(L_{M} - \\sigma^2I)^{1/2}R$\n",
    "\n",
    "$\\sigma^2_{ML} = \\dfrac{1}{D - M}\\sum_{i=M+1}^{D}\\lambda_i$\n",
    "\n",
    "$U_{M}$ - $D \\times M$, eigenvectors of $S$, $L_{M} = diag(\\lambda_1, \\dots, \\lambda_M)$, $R$ - arbitrary orthogonal $M \\times M$ matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-harvest",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$W_{ML} = U_{M}(L_{M} - \\sigma^2I)^{1/2}R$\n",
    "\n",
    "$\\sigma^2_{ML} = \\dfrac{1}{D - M}\\sum_{i=M+1}^{D}\\lambda_i$\n",
    "\n",
    "\n",
    "$U_{M}$ - $D \\times M$, eigenvectors of $S$, $L_{M} = diag(\\lambda_1, \\dots, \\lambda_M)$, $R$ - arbitrary orthogonal $M \\times M$ matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-huntington",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How can we calculate it?\n",
    "- What if $D=M$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-stroke",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$p(x) = N(x| \\mu, C)$, $C = W W^T + \\sigma^2 I$\n",
    "\n",
    "$\\ln p(X|W, \\mu, \\sigma) = -\\dfrac{nD}{2} \\ln(2\\pi) - \\dfrac{n}{2}\\ln |C| \\sum_{i=1}^{n}(x_i-\\mu)C^{-1}(x_i-\\mu)^T$\n",
    "\n",
    "if $D=M$: $U_M = U, L_M = L, UU^T=I$\n",
    "\n",
    "$C = U(L - \\sigma^2I)^{1/2}R R^T (L - \\sigma^2I)^{1/2}U^T + \\sigma^2 I= ULU^T = S \\Rightarrow p(x) = N(x| \\mu, S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-waters",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EM for pPCA\n",
    "\n",
    "$\\ln p(X,Z|\\mu, W, \\sigma^2) = \\sum_{i=1}^{n} [ \\ln p(x_i|z_i) + \\ln(z_i)]$\n",
    "\n",
    "$\\mathbb{E}_{p(Z|X,\\mu, W, \\sigma^2)} \\ln p(X,Z|\\mu, W, \\sigma^2) = \\dots (*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-rapid",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**E step**: recalculate $p(Z|X,\\mu, W, \\sigma^2)$, remember what $p(z|x) = N(z|M^{-1}W(x-\\mu), \\sigma^{-2}M)$:\n",
    "\n",
    "$\\mathbb{E}z_i = M^{-1}W^T(x_i - \\overline{x})$\n",
    "\n",
    "$\\mathbb{E}z_i z_i^T = \\sigma^2 M^{-1}+\\mathbb{E}z_i\\mathbb{E}z_i^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-trick",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**M step**: get $argmax_{W,\\sigma^2}(*)$ :\n",
    "\n",
    "$W_{new} = [\\sum_{i=1}^{n}(x_i - \\overline{x})\\mathbb{E}z_i^T][\\sum_{i=1}^{n}\\mathbb{E}z_iz_i^T]^{-1}$\n",
    "\n",
    "$\\sigma^2_{new} = \\dfrac{1}{nD}\\sum_{i=1}^{n}[||x_i - \\overline{x}||^2 - 2\\mathbb{E}[z_i]^TW_{new}^T(x_i - \\overline{x}) + Tr(\\mathbb{E}[z_i z_i^T] W_{new}^T W_{new})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-diary",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why pPCA?\n",
    "\n",
    "We can:\n",
    "- use EM algorithm\n",
    "- estimate likelihood\n",
    "- generate new samples of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-dylan",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* using EM we'll get W and it's columns need not be orthogonal because of $R$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-sunday",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So, how our EM connected with EM from the lection?\n",
    "<img src=\"pics/EM_lection.jpg\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-maker",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We optimize: $\\ln p(X,Z|\\mu, W, \\sigma^2) = \\sum_{i=1}^{n} [ \\ln p(x_i|z_i) + ln(z_i)]$\n",
    "\n",
    "$\\mathbb{E}_{p(Z|X,\\mu, W, \\sigma^2)} \\ln p(X,Z|\\mu, W, \\sigma^2) = \\dots (*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-immigration",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$L(q,\\theta) = \\int q(z)\\ln p(x|z, \\theta) dz + \\int q(z)\\ln \\frac{p(z)}{q(z)} dz= \\int q(z)[\\ln p(x|z, \\theta) + \\ln p(z)] dz - \\int q(z) \\ln q(z) dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-supplier",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## VAE\n",
    "\n",
    "<img src=\"pics/vae-gaussian.png\" width=800 height=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-excitement",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$L(q, \\theta) = \\mathbb{E}_{q} \\ln p(x|z, \\theta) - KL(q(z)||p(z))$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
